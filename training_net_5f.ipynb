{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_net_5f.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "otwSch5JJnho",
        "colab_type": "code",
        "outputId": "e3dcc7d1-0d19-40f2-b60b-837fe7646ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_QLzh19KAl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Training Parameters\n",
        "learning_rate = 0.01\n",
        "num_steps = 22000\n",
        "batch_size = 120\n",
        "display_step = 1000\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 784 # MNIST data input (img shape: 28*28)\n",
        "num_classes = 10 # MNIST total classes (0-9 digits)\n",
        "dropout = 0.75 # Dropout, probability to keep units\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(tf.float32, [None, num_input])\n",
        "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
        "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F00e7kIyKG_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and square activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def meanpool2d(x, k=2):\n",
        "    \n",
        "    return tf.nn.avg_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                          padding='SAME')\n",
        "\n",
        "# Create model\n",
        "def conv_net(x, weights, biases, dropout):\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv1_ = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    conv1_ = tf.square(conv1_)\n",
        "    conv1 = meanpool2d(conv1_, k=2)\n",
        "    \n",
        "    conv2_ = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    conv2 = meanpool2d(conv2_, k=2)\n",
        "    \n",
        "    fc1_ = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1_, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.square(fc1)\n",
        "    # Apply Dropout\n",
        "    fc1_2 = tf.nn.dropout(fc1, dropout)\n",
        "    \n",
        "    check = {}\n",
        "    check['conv1'] = conv1_\n",
        "    check['conv1mp'] = conv1\n",
        "    check['conv2'] = conv2_\n",
        "    check['conv2mp'] = conv2\n",
        "    check['fc1_reshape'] = fc1_\n",
        "    check['fc1_wb'] = fc1\n",
        "    check['fc1_drp'] = fc1_2\n",
        "\n",
        "    # Output, class prediction\n",
        "    out = tf.add(tf.matmul(fc1_2, weights['out']), biases['out'])\n",
        "    return out, check"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_iN2hNgKzhr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "79b65bba-b945-4bac-c0e4-7e9a03ec58d0"
      },
      "source": [
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    # 5x5 conv, 1 input, 5 outputs\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 5])),\n",
        "    # 5x5 conv, 5 inputs, 10 outputs\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 5, 10])),\n",
        "    # fully connected, 7*7*10 inputs, 128 outputs\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*10, 128])),\n",
        "    # 1024 inputs, 10 outputs (class prediction)\n",
        "    'out': tf.Variable(tf.random_normal([128, num_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([5])),\n",
        "    'bc2': tf.Variable(tf.random_normal([10])),\n",
        "    'bd1': tf.Variable(tf.random_normal([128])),\n",
        "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
        "}\n",
        "\n",
        "\n",
        "# Construct model\n",
        "logits,verify = conv_net(X, weights, biases, keep_prob)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "sess = tf.Session()\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "# Run the initializer\n",
        "sess.run(init)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-034947e91dd9>:29: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXrjN8KsMfzR",
        "colab_type": "code",
        "outputId": "098845a8-95e3-43f0-94fa-ea8c1b8eaf80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "# Start training\n",
        "import numpy as np\n",
        "for step in range(1, num_steps+1):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
        "                                                                 Y: batch_y,\n",
        "                                                                 keep_prob: 1.0})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "\n",
        "# Calculate accuracy for 256 MNIST test images\n",
        "print(\"Testing Accuracy:\", \\\n",
        "    sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
        "                                  Y: mnist.test.labels[:256],\n",
        "                                  keep_prob: 1.0}))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 93628296.0000, Training Accuracy= 0.125\n",
            "Step 1000, Minibatch Loss= 34916.0742, Training Accuracy= 0.950\n",
            "Step 2000, Minibatch Loss= 13126.0791, Training Accuracy= 0.950\n",
            "Step 3000, Minibatch Loss= 3159.7922, Training Accuracy= 0.967\n",
            "Step 4000, Minibatch Loss= 1278.5148, Training Accuracy= 0.975\n",
            "Step 5000, Minibatch Loss= 298.9215, Training Accuracy= 0.983\n",
            "Step 6000, Minibatch Loss= 70.8175, Training Accuracy= 0.992\n",
            "Step 7000, Minibatch Loss= 493.5350, Training Accuracy= 0.983\n",
            "Step 8000, Minibatch Loss= 332.4762, Training Accuracy= 0.983\n",
            "Step 9000, Minibatch Loss= 144.4049, Training Accuracy= 0.992\n",
            "Step 10000, Minibatch Loss= 101.7834, Training Accuracy= 0.975\n",
            "Step 11000, Minibatch Loss= 24.6029, Training Accuracy= 0.992\n",
            "Step 12000, Minibatch Loss= 50.1187, Training Accuracy= 0.975\n",
            "Step 13000, Minibatch Loss= 18.5519, Training Accuracy= 0.992\n",
            "Step 14000, Minibatch Loss= 1.3442, Training Accuracy= 0.992\n",
            "Step 15000, Minibatch Loss= 5.4152, Training Accuracy= 0.983\n",
            "Step 16000, Minibatch Loss= 2.1595, Training Accuracy= 0.983\n",
            "Step 17000, Minibatch Loss= 0.6195, Training Accuracy= 0.992\n",
            "Step 18000, Minibatch Loss= 1.5727, Training Accuracy= 0.975\n",
            "Step 19000, Minibatch Loss= 1.8468, Training Accuracy= 0.992\n",
            "Step 20000, Minibatch Loss= 0.2777, Training Accuracy= 0.992\n",
            "Step 21000, Minibatch Loss= 0.5167, Training Accuracy= 0.983\n",
            "Step 22000, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.9765625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztAqPNtzUjek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
        "    \"\"\"\n",
        "    Freezes the state of a session into a pruned computation graph.\n",
        "\n",
        "    Creates a new computation graph where variable nodes are replaced by\n",
        "    constants taking their current value in the session. The new graph will be\n",
        "    pruned so subgraphs that are not necessary to compute the requested\n",
        "    outputs are removed.\n",
        "    @param session The TensorFlow session to be frozen.\n",
        "    @param keep_var_names A list of variable names that should not be frozen,\n",
        "                          or None to freeze all the variables in the graph.\n",
        "    @param output_names Names of the relevant graph outputs.\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\n",
        "    @return The frozen graph definition.\n",
        "    \"\"\"\n",
        "    graph = session.graph\n",
        "    with graph.as_default():\n",
        "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
        "        output_names = output_names or []\n",
        "        output_names += [v.op.name for v in tf.global_variables()]\n",
        "        input_graph_def = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in input_graph_def.node:\n",
        "                node.device = \"\"\n",
        "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
        "            session, input_graph_def, output_names, freeze_var_names)\n",
        "        return frozen_graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhx7n2egUpDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "8814d038-ec6e-47a8-a4b8-f67b5727dc79"
      },
      "source": [
        "frozen_graph = freeze_session(sess)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-0d92ab84a915>:26: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "INFO:tensorflow:Froze 26 variables.\n",
            "INFO:tensorflow:Converted 26 variables to const ops.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSMAEe38fHqk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e6703c8-deb9-4217-9d6f-9df29c4d9000"
      },
      "source": [
        "from google.colab import files"
        "tf.train.write_graph(frozen_graph, \"/users/sgarge/demos_colab\", \"my_model.pb\", as_text=False)"
        "files.download('/users/sgarge/demos_colab/my_model.pb')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/users/sgarge/demos_colab/my_model.pb'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69FjkJVRQf0P",
        "colab_type": "code",
        "outputId": "2366c3c7-bc02-4b8b-8353-12b28e5bb199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(sess.run(logits, feed_dict={X: np.expand_dims(mnist.test.images[15,:],0),keep_prob: 1.0}))\n",
        "# print(mnist.test.labels[1,:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  429.47992  -600.93146   249.12288 20632.816    4423.942   25896.9\n",
            "   1718.7913  -5013.9365  14425.3     14151.135  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHofCjX4QlfH",
        "colab_type": "code",
        "outputId": "268fdd35-f822-4a8f-c46e-a3e1bf65e837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline\n",
        "idx = 5 # change value to test different image\n",
        "pred = sess.run(tf.argmax(prediction,1), feed_dict={X: np.expand_dims(mnist.test.images[idx,:],0),keep_prob: 1.0})\n",
        "print (\"Prediction:\", pred, \"Actual:\",np.argmax(mnist.test.labels[idx,:],-1))#tf.argmax(np.expand_dims(mnist.test.labels[1],0))#\n",
        "\n",
        "plt.imshow(np.reshape(mnist.test.images[idx,:],(28,28)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: [1] Actual: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd5adec92e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKQGDLiVg0KUEDLqUgEGXEvh/4b8107z4sHEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd5adf920f0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erZb7bh_QtYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "np.save('testim5.npy',mnist.test.images[5,:])\n",
        "files.download('testim5.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOfD1D9AQ_iH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "np.save('weights1.npy',sess.run(weights['wc1']))\n",
        "files.download('weights1.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW5XThlMREKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('weights2.npy',sess.run(weights['wc2']))\n",
        "files.download('weights2.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiHYSVSRRJLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('weights3.npy',sess.run(weights['wd1']))\n",
        "files.download('weights3.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwxwVNtiRTri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('weights4.npy',sess.run(weights['out']))\n",
        "files.download('weights4.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "029FnljMRXrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('bc1.npy',sess.run(biases['bc1']))\n",
        "files.download('bc1.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjO-hPdLRbiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('bc2.npy',sess.run(biases['bc2']))\n",
        "files.download('bc2.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDNa0W7ERfls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('bd1.npy',sess.run(biases['bd1']))\n",
        "files.download('bd1.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyn6Xei1Rjff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('bout.npy',sess.run(biases['out']))\n",
        "files.download('bout.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqOEy_fU0_BR",
        "colab_type": "code",
        "outputId": "b2e1e9a9-9a34-4f9f-cb0b-be4fd09fc79f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1343
        }
      },
      "source": [
        "print(sess.run(weights['wc1']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[-1.15479812e-01  4.16034967e-01 -4.00686592e-01  8.28465939e-01\n",
            "     5.19545019e-01]]\n",
            "\n",
            "  [[-2.59787321e-01  9.04210091e-01 -7.78959468e-02 -4.09770966e-01\n",
            "     1.81714550e-01]]\n",
            "\n",
            "  [[ 2.25485805e-02 -3.54892313e-02  3.57550025e-01  6.68362901e-02\n",
            "     4.74855304e-01]]\n",
            "\n",
            "  [[-5.02884239e-02  2.41878003e-01 -1.52268587e-02  1.19749762e-01\n",
            "    -1.00468755e+00]]\n",
            "\n",
            "  [[ 2.88923532e-01 -2.02378616e-01 -1.45098820e-01  3.28658134e-01\n",
            "     5.59653223e-01]]]\n",
            "\n",
            "\n",
            " [[[ 4.27955955e-01 -1.21412039e+00  1.00577474e+00 -1.27317637e-01\n",
            "    -5.35779953e-01]]\n",
            "\n",
            "  [[ 1.28767863e-01 -2.32767940e-01 -2.59762928e-02 -2.44712755e-02\n",
            "    -5.29364228e-01]]\n",
            "\n",
            "  [[ 1.80617824e-01 -6.16505086e-01 -5.18001199e-01  3.86687696e-01\n",
            "     1.09244987e-01]]\n",
            "\n",
            "  [[-5.69390535e-01  1.60222992e-01  4.18368548e-01 -5.78603506e-01\n",
            "     1.75292969e-01]]\n",
            "\n",
            "  [[ 1.95542634e-01  3.13383192e-01 -9.94491391e-03  1.08717859e-01\n",
            "    -3.71219009e-01]]]\n",
            "\n",
            "\n",
            " [[[-3.22702676e-01  1.57034242e+00 -1.69747323e-01 -4.93205748e-02\n",
            "     4.94472653e-01]]\n",
            "\n",
            "  [[-1.05036296e-01 -6.83993101e-01 -7.56524146e-01 -3.58214110e-01\n",
            "    -2.54033774e-01]]\n",
            "\n",
            "  [[ 1.94417760e-01  6.32994533e-01  6.24008179e-01  8.63677442e-01\n",
            "     1.13036537e+00]]\n",
            "\n",
            "  [[ 3.68542314e-01  5.38533032e-01 -1.50133297e-01 -2.33342960e-01\n",
            "    -1.83970422e-01]]\n",
            "\n",
            "  [[-2.20688641e-01 -5.83352149e-01 -1.29348680e-01 -2.42502943e-01\n",
            "     4.43299681e-01]]]\n",
            "\n",
            "\n",
            " [[[-4.86315638e-02 -7.36715138e-01  6.92482814e-02 -1.28317431e-01\n",
            "     4.41912422e-03]]\n",
            "\n",
            "  [[ 9.80412215e-02  6.28763676e-01  8.33568633e-01  2.38597184e-01\n",
            "     2.59734362e-01]]\n",
            "\n",
            "  [[-8.39311481e-02 -5.45080788e-02 -6.98780596e-01 -1.19694960e+00\n",
            "    -6.44672215e-01]]\n",
            "\n",
            "  [[-4.33961600e-01 -2.36237600e-01  1.09682143e-01  7.36549854e-01\n",
            "     2.74853855e-01]]\n",
            "\n",
            "  [[-4.62150931e-01  2.69434959e-01  1.19329043e-01  2.57432640e-01\n",
            "    -1.24512136e-01]]]\n",
            "\n",
            "\n",
            " [[[-9.24659614e-03 -1.85267739e-02 -5.14094055e-01  9.67636406e-02\n",
            "    -3.36781502e-01]]\n",
            "\n",
            "  [[ 3.80046099e-01 -1.09630346e+00  3.70401293e-01 -9.79939103e-02\n",
            "     1.14766217e-03]]\n",
            "\n",
            "  [[-8.96787524e-01  8.55810642e-01  2.14634478e-01  3.17889392e-01\n",
            "     2.12998509e-01]]\n",
            "\n",
            "  [[ 5.46717286e-01  2.51281977e-01 -3.91696572e-01  5.28632812e-02\n",
            "    -1.98028743e-01]]\n",
            "\n",
            "  [[ 3.09812725e-01 -3.66658688e-01  3.56457502e-01 -4.91407454e-01\n",
            "    -3.68050970e-02]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2Ds18ad85SV",
        "colab_type": "code",
        "outputId": "d6c66099-78ab-49f1-cd3e-cc1769bf2a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2244
        }
      },
      "source": [
        "print(mnist.test.images[1,:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.454902   0.4901961\n",
            " 0.67058825 1.         1.         0.5882353  0.3647059  0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.6627451  0.9921569  0.9921569  0.9921569  0.9921569\n",
            " 0.9921569  0.9921569  0.854902   0.11764707 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.6627451  0.9921569\n",
            " 0.9921569  0.9921569  0.8352942  0.5568628  0.6901961  0.9921569\n",
            " 0.9921569  0.4784314  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.20392159 0.9803922  0.9921569  0.8235295  0.1254902\n",
            " 0.04705883 0.         0.02352941 0.8078432  0.9921569  0.54901963\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.3019608\n",
            " 0.9843138  0.8235295  0.09803922 0.         0.         0.\n",
            " 0.4784314  0.9725491  0.9921569  0.25490198 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.12156864 0.07058824\n",
            " 0.         0.         0.         0.         0.8196079  0.9921569\n",
            " 0.9921569  0.25490198 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.45882356 0.9686275  0.9921569  0.77647066 0.03921569\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.29803923 0.9686275\n",
            " 0.9921569  0.9058824  0.24705884 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.5019608  0.9921569  0.9921569  0.5647059\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.6901961\n",
            " 0.96470594 0.9921569  0.62352943 0.04705883 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.09803922 0.9176471  0.9921569  0.91372555\n",
            " 0.13725491 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.77647066 0.9921569  0.9921569  0.5529412  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.30588236 0.9725491  0.9921569\n",
            " 0.7411765  0.04705883 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.07450981 0.7843138  0.9921569  0.9921569  0.5529412  0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.5254902  0.9921569\n",
            " 0.9921569  0.6784314  0.04705883 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.9725491  0.9921569  0.9921569  0.09803922\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.9725491  0.9921569  0.9921569  0.16862746 0.07843138 0.07843138\n",
            " 0.07843138 0.07843138 0.01960784 0.         0.01960784 0.07843138\n",
            " 0.07843138 0.14509805 0.5882353  0.5882353  0.5882353  0.5764706\n",
            " 0.03921569 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.9725491  0.9921569\n",
            " 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n",
            " 0.65882355 0.56078434 0.6509804  0.9921569  0.9921569  0.9921569\n",
            " 0.9921569  0.9921569  0.9921569  0.9921569  0.48235297 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.68235296 0.9921569  0.9921569  0.9921569\n",
            " 0.9921569  0.9921569  0.9921569  0.9921569  0.9921569  0.9921569\n",
            " 0.9921569  0.9921569  0.97647065 0.9686275  0.9686275  0.6627451\n",
            " 0.45882356 0.45882356 0.22352943 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.46274513 0.48235297 0.48235297 0.48235297 0.6509804\n",
            " 0.9921569  0.9921569  0.9921569  0.60784316 0.48235297 0.48235297\n",
            " 0.16078432 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}